{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TripleGAN - MNIST + Keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 0. Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Numpy .... : 1.19.2\nMatplotlib : 3.3.2\nTensorflow : 2.4.0\nTF Addons  : 0.12.0\nKeras .... : 2.4.3\n"
     ]
    }
   ],
   "source": [
    "# Maths\n",
    "import numpy                                            as np\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib                                       as mp\n",
    "import matplotlib.pyplot                                as pt\n",
    "\n",
    "# Machine / Deep Learning\n",
    "import tensorflow                                       as tf\n",
    "import tensorflow_addons                                as tfa\n",
    "import keras                                            as ks\n",
    "from keras              import models                   as mdls\n",
    "from keras              import layers                   as lyrs\n",
    "from keras.datasets     import mnist                    as mn\n",
    "from keras.utils        import to_categorical           as tc\n",
    "\n",
    "# Versions\n",
    "print( f\"Numpy .... : {np.__version__}\" )\n",
    "print( f\"Matplotlib : {mp.__version__}\" )\n",
    "print( f\"Tensorflow : {tf.__version__}\" )\n",
    "print( f\"TF Addons  : {tfa.__version__}\" )\n",
    "print( f\"Keras .... : {ks.__version__}\" )"
   ]
  },
  {
   "source": [
    "## 1. Set Up Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = ( 28, 28, 1 )\n",
    "classes = 10\n",
    "latent_dim = 100\n",
    "optimizer = tf.optimizers.Adam( 0.0002, 0.5 )\n",
    "losses = ['binary_crossentropy','sparse_categorical_crossentropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Constants\n",
    "\n",
    "noise_dis = 0.2\n",
    "alpha_dis = 0.02\n",
    "\n",
    "\n",
    "# Discriminator Functions\n",
    "\n",
    "def lyrs_Noise( ):\n",
    "    return ks.layers.GaussianNoise( stddev = noise_dis )\n",
    "\n",
    "def lyrs_Dense( units, activation = None ):\n",
    "    return ks.layers.Dense( units = units, activation = activation )\n",
    "\n",
    "def lyrs_WeightNorm( x ):\n",
    "    return tfa.layers.WeightNormalization( x )\n",
    "\n",
    "def lyrs_WeightNormDense( units, activation = None ):\n",
    "    return lyrs_WeightNorm( lyrs_Dense( units = units, activation = activation ) )\n",
    "\n",
    "def lyrs_lReLU( ):\n",
    "    return ks.layers.LeakyReLU( alpha = alpha_dis )"
   ]
  },
  {
   "source": [
    "### 1.0 Set Up Discriminator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Discriminator\n",
    "\n",
    "dr = mdls.Sequential( name = \"Discriminator\" )\n",
    "\n",
    "dr.add( lyrs.Input( shape=( 28, 28, ) ) )\n",
    "dr.add( lyrs.Input( shape=( 10, ) ) )\n",
    "dr.add( lyrs_Noise() )\n",
    "dr.add( lyrs_WeightNormDense( 1000 ) )\n",
    "dr.add( lyrs_lReLU() )\n",
    "dr.add( lyrs_Noise() )\n",
    "dr.add( lyrs_WeightNormDense( 500 ) )\n",
    "dr.add( lyrs_lReLU() )\n",
    "dr.add( lyrs_Noise() )\n",
    "dr.add( lyrs_WeightNormDense( 250 ) )\n",
    "dr.add( lyrs_lReLU() )\n",
    "dr.add( lyrs_Noise() )\n",
    "dr.add( lyrs_WeightNormDense( 250 ) )\n",
    "dr.add( lyrs_lReLU() )\n",
    "dr.add( lyrs_Noise() )\n",
    "dr.add( lyrs_WeightNormDense( 250 ) )\n",
    "dr.add( lyrs_lReLU() )\n",
    "dr.add( lyrs_Noise() )\n",
    "dr.add( lyrs_WeightNormDense( 1, activation = \"sigmoid\" ) )"
   ]
  },
  {
   "source": [
    "### 1.1 Set Up Generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Generator\n",
    "\n",
    "gr = mdls.Sequential( name = \"Generator\" )\n",
    "\n",
    "gr.add( lyrs.Input( shape = ( 10, ) ) )\n",
    "gr.add( lyrs.Input( shape = ( 28, 28, ) ) )\n",
    "gr.add( lyrs.Dense( 500, activation = \"softplus\" ) )\n",
    "gr.add( lyrs.BatchNormalization( ) )\n",
    "gr.add( lyrs.Dense( 500, activation = \"softplus\" ) )\n",
    "gr.add( lyrs.BatchNormalization( ) )\n",
    "gr.add( lyrs.Dense( 784, activation = \"sigmoid\" ) )\n",
    "gr.add( lyrs.BatchNormalization( ) )"
   ]
  },
  {
   "source": [
    "### 1.2 Set Up Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Classifier\n",
    "\n",
    "cr = mdls.Sequential( name = \"Classifier\" )\n",
    "\n",
    "cr.add( lyrs.Input( shape = ( latent_dim, 28, 28 ) ) )\n",
    "cr.add( lyrs.Conv2D( 32, kernel_size = ( 5, 5 ), activation = \"relu\", padding = \"same\" ) )\n",
    "cr.add( lyrs.MaxPooling2D( pool_size = ( 2, 2 ) ) )\n",
    "cr.add( lyrs.Dropout( 0.5 ) )\n",
    "cr.add( lyrs.Conv2D( 64, kernel_size = ( 3, 3 ), activation = \"relu\", padding = \"same\" ) )\n",
    "cr.add( lyrs.Conv2D( 64, kernel_size = ( 3, 3 ), activation = \"relu\", padding = \"same\" ) )\n",
    "cr.add( lyrs.MaxPooling2D( pool_size = ( 2, 2 ) ) )\n",
    "cr.add( lyrs.Dropout( 0.5 ) )\n",
    "cr.add( lyrs.Conv2D( 128, kernel_size = ( 3, 3 ), activation = \"relu\", padding = \"same\" ) )\n",
    "cr.add( lyrs.Conv2D( 128, kernel_size = ( 3, 3 ), activation = \"relu\", padding = \"same\" ) )\n",
    "cr.add( lyrs.GlobalMaxPooling2D() )\n",
    "cr.add( lyrs.Dense( 10, activation = \"softmax\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Discriminator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_30 (InputLayer)        multiple                  0         \n_________________________________________________________________\ngaussian_noise_32 (GaussianN (None, 28, 28)            0         \n_________________________________________________________________\nweight_normalization_31 (Wei (None, 28, 1000)          59001     \n_________________________________________________________________\nleaky_re_lu_26 (LeakyReLU)   (None, 28, 1000)          0         \n_________________________________________________________________\ngaussian_noise_33 (GaussianN (None, 28, 1000)          0         \n_________________________________________________________________\nweight_normalization_32 (Wei (None, 28, 500)           1001501   \n_________________________________________________________________\nleaky_re_lu_27 (LeakyReLU)   (None, 28, 500)           0         \n_________________________________________________________________\ngaussian_noise_34 (GaussianN (None, 28, 500)           0         \n_________________________________________________________________\nweight_normalization_33 (Wei (None, 28, 250)           250751    \n_________________________________________________________________\nleaky_re_lu_28 (LeakyReLU)   (None, 28, 250)           0         \n_________________________________________________________________\ngaussian_noise_35 (GaussianN (None, 28, 250)           0         \n_________________________________________________________________\nweight_normalization_34 (Wei (None, 28, 250)           125751    \n_________________________________________________________________\nleaky_re_lu_29 (LeakyReLU)   (None, 28, 250)           0         \n_________________________________________________________________\ngaussian_noise_36 (GaussianN (None, 28, 250)           0         \n_________________________________________________________________\nweight_normalization_35 (Wei (None, 28, 250)           125751    \n_________________________________________________________________\nleaky_re_lu_30 (LeakyReLU)   (None, 28, 250)           0         \n_________________________________________________________________\ngaussian_noise_37 (GaussianN (None, 28, 250)           0         \n_________________________________________________________________\nweight_normalization_36 (Wei (None, 28, 1)             504       \n=================================================================\nTotal params: 1,563,259\nTrainable params: 782,752\nNon-trainable params: 780,507\n_________________________________________________________________\nModel: \"Generator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_32 (InputLayer)        multiple                  0         \n_________________________________________________________________\ndense_49 (Dense)             (None, 500)               5500      \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 500)               2000      \n_________________________________________________________________\ndense_50 (Dense)             (None, 500)               250500    \n_________________________________________________________________\nbatch_normalization_10 (Batc (None, 500)               2000      \n_________________________________________________________________\ndense_51 (Dense)             (None, 784)               392784    \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 784)               3136      \n=================================================================\nTotal params: 655,920\nTrainable params: 652,352\nNon-trainable params: 3,568\n_________________________________________________________________\nModel: \"Classifier\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_28 (Conv2D)           (None, 100, 28, 32)       22432     \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 50, 14, 32)        0         \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 50, 14, 32)        0         \n_________________________________________________________________\nconv2d_29 (Conv2D)           (None, 50, 14, 64)        18496     \n_________________________________________________________________\nconv2d_30 (Conv2D)           (None, 50, 14, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_12 (MaxPooling (None, 25, 7, 64)         0         \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 25, 7, 64)         0         \n_________________________________________________________________\nconv2d_31 (Conv2D)           (None, 25, 7, 128)        73856     \n_________________________________________________________________\nconv2d_32 (Conv2D)           (None, 25, 7, 128)        147584    \n_________________________________________________________________\nglobal_max_pooling2d_3 (Glob (None, 128)               0         \n_________________________________________________________________\ndense_52 (Dense)             (None, 10)                1290      \n=================================================================\nTotal params: 300,586\nTrainable params: 300,586\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize\n",
    "\n",
    "dr.summary()\n",
    "gr.summary()\n",
    "cr.summary()"
   ]
  },
  {
   "source": [
    "## 2. Setup MNIST Dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "( tr_images, tr_labels ), ( ts_images, ts_labels ) = mn.load_data()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "#all_digits = np.concatenate( [tr_images, ts_images] )\n",
    "all_digits = tr_images[:100]\n",
    "all_digits = ( all_digits.astype( 'float32' ) - 127.5 ) / 127.5\n",
    "all_digits = np.reshape( all_digits, ( -1, 28, 28, 1 ) )\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices( all_digits )\n",
    "dataset = dataset.shuffle( buffer_size=1024 ).batch( batch_size ).prefetch( 32 )"
   ]
  },
  {
   "source": [
    "## 3. Create GAN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "gan = MonoGAN(\n",
    "    discriminator   = dr,\n",
    "    generator       = gr,\n",
    "    dimension       = dimension\n",
    ")"
   ]
  },
  {
   "source": [
    "## 4.  Compile GAN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(\n",
    "    dr_opt          = ks.optimizers.Adam( learning_rate = 0.0003 ),\n",
    "    gr_opt          = ks.optimizers.Adam( learning_rate = 0.0003 ),\n",
    "    loss_function   = ks.losses.BinaryCrossentropy( from_logits = True )\n",
    ")\n"
   ]
  },
  {
   "source": [
    "## 4. Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 66s 69ms/step - loss: 0.8713 - accuracy: 0.8633\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 60s 64ms/step - loss: 0.0243 - accuracy: 0.9948\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.0150 - accuracy: 0.9967\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 46s 49ms/step - loss: 0.0168 - accuracy: 0.9961\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.0163 - accuracy: 0.9964\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x149e3b0a0>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "gan.fit(\n",
    "    dataset,\n",
    "    epochs = epochs,\n",
    "    callbacks = [ MonoGAN_Monitor( image_number = 3, dimension = dimension ) ]\n",
    ")"
   ]
  },
  {
   "source": [
    "## 5. Evaluate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 2s 8ms/step - loss: 0.0508 - accuracy: 0.9910\n",
      "0.9909999966621399\n"
     ]
    }
   ],
   "source": [
    "for i in range( 3 ):\n",
    "    pt.figure( figsize = ( 30, 30 ) )\n",
    "    im = pt.imread( f\"./generated_img_{i}_29.png\" )\n",
    "    pt.imshow( im )"
   ]
  }
 ]
}